{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bphECiUa9zw"
      },
      "source": [
        "# Lab 5: Spam Detection\n",
        "\n",
        "**Deadline**: Monday, Mar 14, 5:00 PM\n",
        "\n",
        "**Late Penalty**: Any work that is submitted between 0 hour and 24 hours past the deadline will receive a 20% grade deduction. No other late work is accepted. Quercus submission time will be used, not your local computer time. You can submit your labs as many times as you want before the deadline, so please submit often and early.\n",
        "\n",
        "**TA**: Hossein Yousefi <hossein.yousefi@mail.utoronto.ca>\n",
        "\n",
        "In this assignment, we will build a recurrent neural network to classify a SMS text message\n",
        "as \"spam\" or \"not spam\". In the process, you will\n",
        "    \n",
        "1. Clean and process text data for machine learning.\n",
        "2. Understand and implement a character-level recurrent neural network.\n",
        "3. Use torchtext to build recurrent neural network models.\n",
        "4. Understand batching for a recurrent neural network, and use torchtext to implement RNN batching.\n",
        "\n",
        "### What to submit\n",
        "\n",
        "Submit a PDF file containing all your code, outputs, and write-up. You can produce a PDF of your Google Colab file by going to File > Print and then save as PDF. The Colab instructions have more information (.html files are also acceptable).\n",
        "\n",
        "Do not submit any other files produced by your code.\n",
        "\n",
        "Include a link to your colab file in your submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWiUqJJTa9z6"
      },
      "source": [
        "## Colab Link\n",
        "\n",
        "Include a link to your Colab file here. If you would like the TA to look at your\n",
        "Colab file in case your solutions are cut off, **please make sure that your Colab\n",
        "file is publicly accessible at the time of submission**.\n",
        "\n",
        "Colab Link: https://colab.research.google.com/drive/1BYR00UOS3nEUMWShfs3fHK3jOJR3r7eU?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aritejh KR Goil, 1008682971"
      ],
      "metadata": {
        "id": "Zple6yIOSoUY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HgfNOUaPa9z8",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jLI9LBa90C"
      },
      "source": [
        "## Part 1. Data Cleaning [15 pt]\n",
        "\n",
        "We will be using the \"SMS Spam Collection Data Set\" available at http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
        "\n",
        "There is a link to download the \"Data Folder\" at the very top of the webpage. Download the zip file, unzip it, and upload the file `SMSSpamCollection` to Colab.    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIsghI2KutLl",
        "outputId": "2c1de0af-bc50-445d-9f22-07a6eab10cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-10 18:49:11--  http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203415 (199K) [application/x-httpd-php]\n",
            "Saving to: ‘smsspamcollection.zip’\n",
            "\n",
            "smsspamcollection.z 100%[===================>] 198.65K  1.18MB/s    in 0.2s    \n",
            "\n",
            "2022-03-10 18:49:11 (1.18 MB/s) - ‘smsspamcollection.zip’ saved [203415/203415]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/smsspamcollection.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A2z_7kTvGY1",
        "outputId": "b58251ab-515b-4dcf-f2b5-9a6f34b033ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/smsspamcollection.zip\n",
            "  inflating: SMSSpamCollection       \n",
            "  inflating: readme                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSuF7C_Ga90E"
      },
      "source": [
        "### Part (a) [2 pt]\n",
        "\n",
        "Open up the file in Python, and print out one example of a spam SMS, and one example of a non-spam SMS.\n",
        "\n",
        "What is the label value for a spam message, and what is the label value for a non-spam message?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the readme, label value for spam is 'spam' and label for non-spam message is 'ham'."
      ],
      "metadata": {
        "id": "p4LmbLdXzTiU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_IfXHeTa90F",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "dfc5616f-36ff-4e03-fe83-96aa426d90ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam example: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "non-spam example: U dun say so early hor... U c already then say...\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "smsfile = open('SMSSpamCollection')\n",
        "sms = csv.reader(smsfile, delimiter = \"\\t\")\n",
        "for line in sms: \n",
        "  if line[0] == 'spam':\n",
        "    print (f\"spam example: {line[1]}\")\n",
        "    break\n",
        "for line in sms: \n",
        "  if line[0] == 'ham':\n",
        "    print (f\"non-spam example: {line[1]}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AukA6vMVa90d"
      },
      "source": [
        "### Part (b) [1 pt]\n",
        "\n",
        "How many spam messages are there in the data set?\n",
        "How many non-spam messages are there in the data set?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgsqyemVa90e",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "cf772cf4-9500-4a40-d7fc-3ec191d133a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are 746 spam messages and 4822 non-spam messages\n"
          ]
        }
      ],
      "source": [
        "spam, ham = 0,0\n",
        "\n",
        "for line in sms:\n",
        "  if line[0] == 'spam':\n",
        "    spam += 1\n",
        "  else:\n",
        "    ham +=1\n",
        "\n",
        "print(f\"there are {spam} spam messages and {ham} non-spam messages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1WXxVt6a90h"
      },
      "source": [
        "### Part (c) [4 pt]\n",
        "\n",
        "We will be using the package `torchtext` to load, process, and batch the data.\n",
        "A tutorial to torchtext is available below. This tutorial uses the same\n",
        "Sentiment140 data set that we explored during lecture.\n",
        "\n",
        "https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8\n",
        "\n",
        "Unlike what we did during lecture, we will be building a **character level RNN**.\n",
        "That is, we will treat each **character** as a token in our sequence,\n",
        "rather than each **word**.\n",
        "\n",
        "Identify two advantage and two disadvantage of modelling SMS text\n",
        "messages as a sequence of characters rather than a sequence of words."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two advantages of modelling SMS as a sequence of characters is the ability to work with misspelled words and a smaller vocabulary. Two disadvantages would be the additional complexity due to increased numbers of tokens, making it more complex, and the lowered accuracy."
      ],
      "metadata": {
        "id": "CT6HxL6O0-YR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie_D0bv9a90k"
      },
      "source": [
        "### Part (d) [1 pt]\n",
        "\n",
        "We will be loading our data set using `torchtext.data.TabularDataset`. The\n",
        "constructor will read directly from the `SMSSpamCollection` file. \n",
        "\n",
        "For the data file to be read successfuly, we\n",
        "need to specify the **fields** (columns) in the file. \n",
        "In our case, the dataset has two fields: \n",
        "\n",
        "- a text field containing the sms messages,\n",
        "- a label field which will be converted into a binary label.\n",
        "\n",
        "Split the dataset into `train`, `valid`, and `test`. Use a 60-20-20 split.\n",
        "You may find this torchtext API page helpful:\n",
        "https://torchtext.readthedocs.io/en/latest/data.html#dataset\n",
        "\n",
        "Hint: There is a `Dataset` method that can perform the random split for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_Y6Puz9a90l",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "4fe69dae-2bfa-41ff-d25f-508bb6d092ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "0\n",
            "There are 3343 data samples in the train set, 1115 in val set, and 1114 in the test set\n"
          ]
        }
      ],
      "source": [
        "import torchtext\n",
        "import random\n",
        "text_field = torchtext.legacy.data.Field(sequential=True,      # text sequence\n",
        "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
        "                                  include_lengths=True, # to track the length of sequences, for batching\n",
        "                                  batch_first=True,\n",
        "                                  use_vocab=True)       # to turn each character into an integer index\n",
        "label_field = torchtext.legacy.data.Field(sequential=False,    # not a sequence\n",
        "                                   use_vocab=False,     # don't need to track vocabulary\n",
        "                                   is_target=True,      \n",
        "                                   batch_first=True,\n",
        "                                   preprocessing=lambda x: int(x == 'spam')) # convert text to 0 and 1\n",
        "\n",
        "fields = [('label', label_field), ('sms', text_field)]\n",
        "dataset = torchtext.legacy.data.TabularDataset(\"SMSSpamCollection\", # name of the file\n",
        "                                        \"tsv\",               # fields are separated by a tab\n",
        "                                        fields)\n",
        "\n",
        "print(dataset[0].sms)\n",
        "print(dataset[0].label)\n",
        "\n",
        "train_data, val_data, test_data = dataset.split(split_ratio = [0.6, 0.2, 0.2], random_state=random.seed(99))\n",
        "print(f'There are {len(train_data)} data samples in the train set, {len(val_data)} in val set, and {len(test_data)} in the test set')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6nP0Ks_a90o"
      },
      "source": [
        "### Part (e) [2 pt]\n",
        "\n",
        "You saw in part (b) that there are many more non-spam messages than spam messages.\n",
        "This **imbalance** in our training data will be problematic for training.\n",
        "We can fix this disparity by duplicating spam messages in the training set,\n",
        "so that the training set is roughly **balanced**.\n",
        "\n",
        "Explain why having a balanced training set is helpful for training our neural network.\n",
        "\n",
        "Note: if you are not sure, try removing the below code and train your mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FWvx9_rka90p",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# save the original training examples\n",
        "old_train_examples = train_data.examples\n",
        "# get all the spam messages in `train`\n",
        "train_spam = []\n",
        "for item in train_data.examples:\n",
        "    if item.label == 1:\n",
        "        train_spam.append(item)\n",
        "# duplicate each spam message 6 more times\n",
        "train_data.examples = old_train_examples + train_spam * 6"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to have a balanced dataset as a dominant class can teach the model to predict that specific class overwhelmingly, resulting in a bias. Considering a baseline of predicting non-spam for everything, we would get an accuracy of 0.866 without class balancing. "
      ],
      "metadata": {
        "id": "chAWTIep6vZ-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7eUmBEva90r"
      },
      "source": [
        "### Part (f) [1 pt]\n",
        "\n",
        "We need to build the vocabulary on the training data by running the below code.\n",
        "This finds all the possible character tokens in the training set.\n",
        "\n",
        "Explain what the variables `text_field.vocab.stoi` and `text_field.vocab.itos` represent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CQM8flKa90s",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "63c6509f-e510-4b23-87a1-eabce1e225ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<bound method Vocab._default_unk_index of <torchtext.legacy.vocab.Vocab object at 0x7f020d412090>>, {'<unk>': 0, '<pad>': 1, ' ': 2, 'e': 3, 'o': 4, 't': 5, 'a': 6, 'n': 7, 'r': 8, 'i': 9, 's': 10, 'l': 11, 'u': 12, '0': 13, 'h': 14, 'd': 15, '.': 16, 'm': 17, 'c': 18, 'y': 19, 'w': 20, 'p': 21, 'g': 22, '1': 23, 'f': 24, '2': 25, 'b': 26, 'T': 27, '8': 28, 'k': 29, 'E': 30, '5': 31, 'v': 32, 'S': 33, 'I': 34, 'C': 35, 'O': 36, 'N': 37, '4': 38, '7': 39, 'A': 40, 'x': 41, '3': 42, '6': 43, 'R': 44, '!': 45, '9': 46, ',': 47, 'P': 48, 'U': 49, 'W': 50, 'M': 51, 'L': 52, 'H': 53, 'D': 54, 'G': 55, 'B': 56, 'F': 57, 'Y': 58, '/': 59, '?': 60, \"'\": 61, '£': 62, '-': 63, '&': 64, ':': 65, 'X': 66, 'z': 67, 'V': 68, 'j': 69, 'K': 70, '*': 71, ')': 72, 'J': 73, ';': 74, '+': 75, '(': 76, 'Q': 77, '\"': 78, '#': 79, 'q': 80, '@': 81, 'Z': 82, '=': 83, '>': 84, 'ü': 85, '<': 86, '$': 87, 'Ü': 88, '_': 89, '\\x92': 90, '%': 91, '‘': 92, '|': 93, '[': 94, ']': 95, '…': 96, '\\x93': 97, 'é': 98, '\\t': 99, '\\n': 100, '~': 101, '\\x94': 102, '\\x96': 103, '–': 104, '’': 105, '\\\\': 106, '\\x91': 107, '»': 108, 'É': 109, '—': 110, '┾': 111, '〨': 112, '鈥': 113})\n",
            "['<unk>', '<pad>', ' ', 'e', 'o', 't', 'a', 'n', 'r', 'i', 's', 'l', 'u', '0', 'h', 'd', '.', 'm', 'c', 'y', 'w', 'p', 'g', '1', 'f', '2', 'b', 'T', '8', 'k', 'E', '5', 'v', 'S', 'I', 'C', 'O', 'N', '4', '7', 'A', 'x', '3', '6', 'R', '!', '9', ',', 'P', 'U', 'W', 'M', 'L', 'H', 'D', 'G', 'B', 'F', 'Y', '/', '?', \"'\", '£', '-', '&', ':', 'X', 'z', 'V', 'j', 'K', '*', ')', 'J', ';', '+', '(', 'Q', '\"', '#', 'q', '@', 'Z', '=', '>', 'ü', '<', '$', 'Ü', '_', '\\x92', '%', '‘', '|', '[', ']', '…', '\\x93', 'é', '\\t', '\\n', '~', '\\x94', '\\x96', '–', '’', '\\\\', '\\x91', '»', 'É', '—', '┾', '〨', '鈥']\n"
          ]
        }
      ],
      "source": [
        "text_field.build_vocab(train_data)\n",
        "print(text_field.vocab.stoi)\n",
        "print(text_field.vocab.itos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`text_field.vocab.stoi` returns a collections.defaultdict instance with mapping each token string (character) to a numerical identifier. \n",
        "`text_field.vocab.itos` returns a list of token strings indexed by their numerical identifiers."
      ],
      "metadata": {
        "id": "kafImNuT7pfD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC8WVE8Ua90u"
      },
      "source": [
        "### Part (g) [2 pt]\n",
        "\n",
        "The tokens `<unk>` and `<pad>` were not in our SMS text messages.\n",
        "What do these two values represent?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`<unk>` represents unknown tokens.\n",
        "\n",
        "`<pad>` is used to pad batches and make them the same length"
      ],
      "metadata": {
        "id": "YjPMs9Cu8WoS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff5CNk7Qa90y"
      },
      "source": [
        "### Part (h) [2 pt]\n",
        "\n",
        "Since text sequences are of variable length, `torchtext` provides a `BucketIterator` data loader,\n",
        "which batches similar length sequences together. The iterator also provides functionalities to\n",
        "pad sequences automatically.\n",
        "\n",
        "Take a look at 10 batches in `train_iter`. What is the maximum length of the\n",
        "input sequence in each batch? How many `<pad>` tokens are used in each of the 10\n",
        "batches?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V8N8qLWOa90y",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "train_iter = torchtext.legacy.data.BucketIterator(train_data,\n",
        "                                           batch_size=32,\n",
        "                                           sort_key=lambda x: len(x.sms), # to minimize padding\n",
        "                                           sort_within_batch=True,        # sort within each batch\n",
        "                                           repeat=False)                  # repeat the iterator for many epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qwz-rOaha902",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "1cffeaae-8d5d-4d23-d17b-b386c8100f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: 1 | Length: 28 | Pad: 18\n",
            "Batch: 2 | Length: 80 | Pad: 59\n",
            "Batch: 3 | Length: 35 | Pad: 15\n",
            "Batch: 4 | Length: 147 | Pad: 30\n",
            "Batch: 5 | Length: 155 | Pad: 23\n",
            "Batch: 6 | Length: 160 | Pad: 5\n",
            "Batch: 7 | Length: 77 | Pad: 59\n",
            "Batch: 8 | Length: 163 | Pad: 15\n",
            "Batch: 9 | Length: 34 | Pad: 22\n",
            "Batch: 10 | Length: 89 | Pad: 64\n"
          ]
        }
      ],
      "source": [
        "for i, batch in enumerate(train_iter):\n",
        "  if i == 10:\n",
        "    break\n",
        "  else:\n",
        "    pad = sum([True for j in range(len(batch.sms[0][i])) for i in range(len(batch.sms[1])) if batch.sms[0][i][j] == 1])\n",
        "    print(f'Batch: {i+1} | Length: {int(batch.sms[1][0])} | Pad: {pad}')\n",
        "    # max length is at the start of the batch\n",
        "    # pad is calculated by checking each value in sms[0][i][j],\n",
        "    # which is essentially checking each element if it is == 1, the numerical identifier of pad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7HnqP6_a904"
      },
      "source": [
        "## Part 2. Model Building [8 pt]\n",
        "\n",
        "Build a recurrent neural network model, using an architecture of your choosing. \n",
        "Use the one-hot embedding of each character as input to your recurrent network.\n",
        "Use one or more fully-connected layers to make the prediction based on your\n",
        "recurrent network output.\n",
        "\n",
        "Instead of using the RNN output value for the final token, another often used\n",
        "strategy is to max-pool over the entire output array. That is, instead of calling\n",
        "something like:\n",
        "\n",
        "```\n",
        "out, _ = self.rnn(x)\n",
        "self.fc(out[:, -1, :])\n",
        "```\n",
        "\n",
        "where `self.rnn` is an `nn.RNN`, `nn.GRU`, or `nn.LSTM` module, and `self.fc` is a \n",
        "fully-connected \n",
        "layer, we use:\n",
        "\n",
        "```\n",
        "out, _ = self.rnn(x)\n",
        "self.fc(torch.max(out, dim=1)[0])\n",
        "```\n",
        "\n",
        "This works reasonably in practice. An even better alternative is to concatenate the\n",
        "max-pooling and average-pooling of the RNN outputs:\n",
        "\n",
        "```\n",
        "out, _ = self.rnn(x)\n",
        "out = torch.cat([torch.max(out, dim=1)[0], \n",
        "                 torch.mean(out, dim=1)], dim=1)\n",
        "self.fc(out)\n",
        "```\n",
        "\n",
        "We encourage you to try out all these options. The way you pool the RNN outputs\n",
        "is one of the \"hyperparameters\" that you can choose to tune later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHl1p_Wwa905",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "2f39f67f-72c6-4925-eecc-fbcac29ce6f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]])\n"
          ]
        }
      ],
      "source": [
        "# You might find this code helpful for obtaining\n",
        "# PyTorch one-hot vectors.\n",
        "\n",
        "ident = torch.eye(10)\n",
        "print(ident[0]) # one-hot vector\n",
        "print(ident[1]) # one-hot vector\n",
        "x = torch.tensor([[1, 2], [3, 4]])\n",
        "print(ident[x]) # one-hot vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LTQ7zFka909",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "cf18e3b7-f112-4fa3-a66e-c4edb82cd3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "114\n"
          ]
        }
      ],
      "source": [
        "input_size = len(text_field.vocab.itos)\n",
        "print (input_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "I tried using LSTM, but couldnt get them to work. I have decided to leave them here so i can get feedback on how LSTM implementations can work.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SpamRNN(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        self.name = \"SpamRNN\"\n",
        "        super(SpamRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(114, 114)\n",
        "        self.rnn = nn.LSTM(114,\n",
        "                           64,\n",
        "                           num_layers= 2,\n",
        "                           bidirectional=True)\n",
        "                          #  batch_first=True)\n",
        "        self.fc1 = nn.Linear(64,32)\n",
        "\n",
        "        # model.add(LSTM(128,input_shape=(embedded_docs.shape),activation='relu',return_sequences=True))\n",
        "        # self.embed = torch.eye(input_size)\n",
        "        # self.rnn1 = nn.Sequential(nn.LSTM(input_size, 256, activation='relu'), #nonlinearity = 'relu'),\n",
        "        #                           nn.BatchNorm2d(256)) #batch_first=True),\n",
        "        # self.rnn2 = nn.Sequential(nn.LSTM(256, 64, dropout = dropout),#nonlinearity = 'relu'), \n",
        "        #                           nn.BatchNorm2d(64))\n",
        "        # self.rnn3 = nn.Sequential(nn.LSTM(64, 16, dropout = dropout), #nonlinearity = 'relu'),\n",
        "        #                           nn.BatchNorm2d(16))                         \n",
        "\n",
        "        self.fc2 = nn.Linear(32, 1) # as this is a binary classifier, 1 output is sufficient\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_length):\n",
        "        # Look up the embedding\n",
        "        embedded = self.embedding(text)      \n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length)        # Forward propagate the RNN\n",
        "        packed_output, (hidden, cell) = self.rnn(packed)\n",
        "        # hidden = hidden.view(-1, 64) #reshaping the data for Dense layer next\n",
        "        # out = nn.ReLU(hidden)\n",
        "\n",
        "        combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        out = combined.squeeze(0).view(-1)\n",
        "        out = nn.ReLU(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "\n",
        "\n",
        "        # output, (hidden, _) = self.rnn1(x)\n",
        "        # output, (hidden, _) = self.rnn2(x)\n",
        "        # output, (hidden, _) = self.rnn3(x)\n",
        "        # output, (hidden, _) = self.dropout(x)        \n",
        "        # out = torch.max(x, dim=1)[0]\n",
        "        # # Pass the output of the last step to the classifier\n",
        "        # out = self.fc(out)\n",
        "        # text_fea = torch.squeeze(text_fea, 1)\n",
        "        # text_out = torch.sigmoid(text_fea)\n",
        "        # return out"
      ],
      "metadata": {
        "id": "lcakDtCaGHwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, hidden_size, dropout):\n",
        "        self.name = \"RNN\"\n",
        "        super(RNN, self).__init__()\n",
        "        self.emb = torch.eye(input_size).to(device)\n",
        "        self.rnn = nn.RNN(114, hidden_size, batch_first=True) \n",
        "        self.fc1 = nn.Linear(hidden_size * 2, 32) # *2 is due to concatenation\n",
        "        self.fc2 = nn.Linear(32,1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.hidden_size = hidden_size\n",
        "    def forward(self, x):\n",
        "        x = self.emb[x]\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(device)\n",
        "        out, hidden = self.rnn(x, h0)\n",
        "        out = torch.cat([torch.max(out, dim=1)[0], \n",
        "                 torch.mean(out, dim=1)], dim=1)        \n",
        "        out = self.dropout(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out.squeeze()"
      ],
      "metadata": {
        "id": "d99NyiClTa_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKIYPl_Ba90_"
      },
      "source": [
        "## Part 3. Training [16 pt]\n",
        "\n",
        "### Part (a) [4 pt]\n",
        "\n",
        "Complete the `get_accuracy` function, which will compute the\n",
        "accuracy (rate) of your model across a dataset (e.g. validation set).\n",
        "You may modify `torchtext.data.BucketIterator` to make your computation\n",
        "faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pvNfhGD6a91A",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, iter):\n",
        "    \"\"\" Compute the accuracy of the `model` across a dataset `data`\n",
        "    \n",
        "    Example usage:\n",
        "    \n",
        "    >>> model = MyRNN() # to be defined\n",
        "    >>> get_accuracy(model, valid) # the variable `valid` is from above\n",
        "    \"\"\"\n",
        "    correct, total= 0, 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for idx, (sms, label) in enumerate(iter):\n",
        "        label = label.to(device)\n",
        "        logits = model(sms[0].to(device))\n",
        "        pred = (torch.sigmoid(logits) > 0.5)\n",
        "        correct += (pred == label).sum().item()\n",
        "        total += label.shape[0]\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlxlcAC1a91C"
      },
      "source": [
        "### Part (b) [4 pt]\n",
        "\n",
        "Train your model. Plot the training curve of your final model. \n",
        "Your training curve should have the training/validation loss and\n",
        "accuracy plotted periodically.\n",
        "\n",
        "Note: Not all of your batches will have the same batch size.\n",
        "In particular, if your training set does not divide evenly by\n",
        "your batch size, there will be a batch that is smaller than\n",
        "the rest. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMlBSJX97IYb",
        "outputId": "5a701aa9-a097-415f-d9cf-ea9d6849060e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CVtf7CJCa91D",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def train_rnn(model, train, valid, num_epochs, batch_size, learning_rate, hidden_layers, dropout, plot = True):\n",
        "    model = model.to(device)\n",
        "    torch.manual_seed(4)\n",
        "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5, verbose= True, min_lr=1e-6)\n",
        "    \n",
        "    model_path = f\"model_{model.name}_bs{batch_size}_lr{learning_rate}_epochs{num_epochs}_hidden{hidden_layers}_dropout{dropout}\"\n",
        "    os.makedirs(os.path.join(\"/content/backup\",model_path), exist_ok = True)\n",
        "    \n",
        "    epochs, losses, val_losses, train_acc, val_acc = [], [], [], [], []\n",
        "    train_iter = torchtext.legacy.data.BucketIterator(\n",
        "                    train,\n",
        "                    batch_size=batch_size,\n",
        "                    sort_key=lambda x: len(x.sms), # to minimize padding\n",
        "                    sort_within_batch=True,        # sort within each batch\n",
        "                    repeat=False)                  # repeat the iterator for many epochs\n",
        "    valid_iter = torchtext.legacy.data.BucketIterator(\n",
        "                    valid,\n",
        "                    batch_size=batch_size,\n",
        "                    sort_key=lambda x: len(x.sms), # to minimize padding\n",
        "                    sort_within_batch=True,        # sort within each batch\n",
        "                    repeat=False)                  # repeat the iterator for many epochs\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss_per_epoch, train_batches, val_loss_per_epoch, val_batches = 0.0, 0.0, 0.0, 0.0\n",
        "        for sms, label in train_iter:\n",
        "            # sms =  sms.to(device)\n",
        "            sms_device = sms[0].to(device) \n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(sms_device)# sms[1])\n",
        "            loss = criterion(pred, label.float())\n",
        "            train_loss_per_epoch += loss.item()\n",
        "            train_batches +=1\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        for sms, label in valid_iter:\n",
        "            sms_device, label = sms[0].to(device), label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(sms_device)\n",
        "            val_loss = criterion(pred, label.float())\n",
        "            val_loss_per_epoch += val_loss.item()\n",
        "            val_batches +=1\n",
        "\n",
        "        epochs.append(epoch)            \n",
        "        losses.append(float(train_loss_per_epoch)/train_batches)\n",
        "        train_acc.append(get_accuracy(model, train_iter))\n",
        "        val_losses.append(float(val_loss_per_epoch)/val_batches) \n",
        "        val_acc.append(get_accuracy(model, valid_iter))\n",
        "        scheduler.step(val_acc[-1])\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}: Training loss {losses[-1]:.3f} , Val loss {val_losses[-1]:.3f} , Training accuracy {train_acc[-1]:.3f} , Val accuracy {val_acc[-1]:.3f}\")\n",
        "        if val_acc[-1]==max(val_acc):\n",
        "            torch.save(model.state_dict(), os.path.join(\"/content/backup\", model_path, \"best.pth\"))\n",
        "            print (\"best model saved\")\n",
        "        if epoch % 10 == 0:\n",
        "            torch.save(model.state_dict(), os.path.join(\"/content/backup\", model_path, f\"{epoch}.pth\"))\n",
        "            print (\"checkpoint model saved\")\n",
        "\n",
        "    print (f\"Best training accuracy is {max(train_acc)}\")\n",
        "    print (f\"Best val accuracy is {max(val_acc)}\")\n",
        "    if plot: # plotting\n",
        "      plt.title(\"Loss Curve\")\n",
        "      plt.plot(epochs, losses, label=\"Train\")\n",
        "      plt.plot(epochs, val_losses, label=\"Validation\")\n",
        "      plt.xlabel(\"Iterations\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.legend(loc='best')\n",
        "      plt.show()\n",
        "\n",
        "      plt.title(\"Accuracy Curve\")\n",
        "      plt.plot(epochs, train_acc, label=\"Train\")\n",
        "      plt.plot(epochs, val_acc, label=\"Validation\")\n",
        "      plt.xlabel(\"Iterations\")\n",
        "      plt.ylabel(\"Accuracy\")\n",
        "      plt.legend(loc='best')\n",
        "      plt.show()       "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layers = 64\n",
        "dropout = 0.5\n",
        "batch_size = 32\n",
        "learning_rate = 0.001 \n",
        "\n",
        "SimpleRNN = RNN(hidden_layers, dropout)\n",
        "\n",
        "train_rnn(SimpleRNN, train_data, val_data, num_epochs= 30, batch_size= batch_size, learning_rate=learning_rate, hidden_layers = hidden_layers, dropout = dropout, plot = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ITfPjxOtR50x",
        "outputId": "6ca8fc88-d9cf-49c5-af35-5c6c0a2b973b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30: Training loss 0.494 , Val loss 0.161 , Training accuracy 0.947 , Val accuracy 0.961\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/30: Training loss 0.149 , Val loss 0.200 , Training accuracy 0.967 , Val accuracy 0.948\n",
            "Epoch 3/30: Training loss 0.120 , Val loss 0.176 , Training accuracy 0.974 , Val accuracy 0.956\n",
            "Epoch 4/30: Training loss 0.092 , Val loss 0.273 , Training accuracy 0.967 , Val accuracy 0.934\n",
            "Epoch 5/30: Training loss 0.084 , Val loss 0.126 , Training accuracy 0.984 , Val accuracy 0.965\n",
            "best model saved\n",
            "Epoch 6/30: Training loss 0.066 , Val loss 0.147 , Training accuracy 0.990 , Val accuracy 0.967\n",
            "best model saved\n",
            "Epoch 7/30: Training loss 0.049 , Val loss 0.162 , Training accuracy 0.992 , Val accuracy 0.961\n",
            "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 8/30: Training loss 0.048 , Val loss 0.134 , Training accuracy 0.995 , Val accuracy 0.967\n",
            "best model saved\n",
            "Epoch 9/30: Training loss 0.029 , Val loss 0.162 , Training accuracy 0.998 , Val accuracy 0.966\n",
            "Epoch 10/30: Training loss 0.023 , Val loss 0.160 , Training accuracy 0.998 , Val accuracy 0.967\n",
            "best model saved\n",
            "Epoch 11/30: Training loss 0.023 , Val loss 0.168 , Training accuracy 0.999 , Val accuracy 0.967\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch    12: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 12/30: Training loss 0.016 , Val loss 0.195 , Training accuracy 0.999 , Val accuracy 0.967\n",
            "best model saved\n",
            "Epoch 13/30: Training loss 0.012 , Val loss 0.172 , Training accuracy 0.999 , Val accuracy 0.969\n",
            "best model saved\n",
            "Epoch 14/30: Training loss 0.011 , Val loss 0.212 , Training accuracy 0.998 , Val accuracy 0.964\n",
            "Epoch 15/30: Training loss 0.012 , Val loss 0.206 , Training accuracy 1.000 , Val accuracy 0.969\n",
            "best model saved\n",
            "Epoch    16: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 16/30: Training loss 0.010 , Val loss 0.198 , Training accuracy 1.000 , Val accuracy 0.967\n",
            "Epoch 17/30: Training loss 0.008 , Val loss 0.196 , Training accuracy 1.000 , Val accuracy 0.968\n",
            "Epoch 18/30: Training loss 0.006 , Val loss 0.197 , Training accuracy 1.000 , Val accuracy 0.966\n",
            "Epoch 19/30: Training loss 0.005 , Val loss 0.222 , Training accuracy 1.000 , Val accuracy 0.969\n",
            "best model saved\n",
            "Epoch    20: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 20/30: Training loss 0.006 , Val loss 0.205 , Training accuracy 1.000 , Val accuracy 0.966\n",
            "Epoch 21/30: Training loss 0.006 , Val loss 0.188 , Training accuracy 1.000 , Val accuracy 0.969\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 22/30: Training loss 0.004 , Val loss 0.203 , Training accuracy 1.000 , Val accuracy 0.967\n",
            "Epoch 23/30: Training loss 0.004 , Val loss 0.224 , Training accuracy 1.000 , Val accuracy 0.968\n",
            "Epoch    24: reducing learning rate of group 0 to 3.1250e-05.\n",
            "Epoch 24/30: Training loss 0.005 , Val loss 0.208 , Training accuracy 1.000 , Val accuracy 0.969\n",
            "best model saved\n",
            "Epoch 25/30: Training loss 0.004 , Val loss 0.175 , Training accuracy 1.000 , Val accuracy 0.968\n",
            "Epoch 26/30: Training loss 0.003 , Val loss 0.215 , Training accuracy 1.000 , Val accuracy 0.967\n",
            "Epoch 27/30: Training loss 0.003 , Val loss 0.232 , Training accuracy 1.000 , Val accuracy 0.967\n",
            "Epoch    28: reducing learning rate of group 0 to 1.5625e-05.\n",
            "Epoch 28/30: Training loss 0.003 , Val loss 0.205 , Training accuracy 1.000 , Val accuracy 0.967\n",
            "Epoch 29/30: Training loss 0.004 , Val loss 0.213 , Training accuracy 1.000 , Val accuracy 0.968\n",
            "Epoch 30/30: Training loss 0.004 , Val loss 0.226 , Training accuracy 1.000 , Val accuracy 0.967\n",
            "Best training accuracy is 1.0\n",
            "Best val accuracy is 0.968609865470852\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE3eRkDAa91F"
      },
      "source": [
        "### Part (c) [4 pt]\n",
        "\n",
        "Choose at least 4 hyperparameters to tune. Explain how you tuned the hyperparameters.\n",
        "You don't need to include your training curve for every model you trained.\n",
        "Instead, explain what hyperparemters you tuned, what the best validation accuracy was,\n",
        "and the reasoning behind the hyperparameter decisions you made.\n",
        "\n",
        "For this assignment, you should tune more than just your learning rate and epoch. \n",
        "Choose at least 2 hyperparameters that are unrelated to the optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 4 hyperparameters tuned are:\n",
        "\n",
        "1.   Initial learning rate: 0.001, 0.0005, 0.0001 \n",
        "2.   Batch size: 32 or 64\n",
        "3.   Dropout rates: 0.1, 0.25 and 0.5\n",
        "4.   Hidden layers: 64, 128, 256\n",
        "\n",
        "Number of epochs is fixed at 20 as model stabilises quickly.\n"
      ],
      "metadata": {
        "id": "j8XUrlo_KPCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first, we experiment with batch size and learning rate with the same model to find the best combination\n",
        "for batch_size in [32, 64]:\n",
        "  for learning_rate in [0.001,0.0005,0.0001]:\n",
        "    print (\"--------------------------------------------------------\")\n",
        "    print (f\"Training bs{batch_size} lr{learning_rate} \")\n",
        "    SimpleRNN = RNN(128,0.5)\n",
        "    train_rnn(SimpleRNN, train_data, val_data, num_epochs= 20, batch_size= batch_size, learning_rate=learning_rate, hidden_layers = 128, dropout = 0.5, plot = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdw-JI6gnlpp",
        "outputId": "03af2327-5bc9-41b0-f221-da37c65c12fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------\n",
            "Training bs32 lr0.001 \n",
            "Epoch 1/20: Training loss 0.446 , Val loss 0.133 , Training accuracy 0.946 , Val accuracy 0.961\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.135 , Val loss 0.176 , Training accuracy 0.970 , Val accuracy 0.952\n",
            "Epoch 3/20: Training loss 0.107 , Val loss 0.128 , Training accuracy 0.977 , Val accuracy 0.965\n",
            "best model saved\n",
            "Epoch 4/20: Training loss 0.086 , Val loss 0.173 , Training accuracy 0.979 , Val accuracy 0.951\n",
            "Epoch 5/20: Training loss 0.066 , Val loss 0.120 , Training accuracy 0.989 , Val accuracy 0.961\n",
            "Epoch 6/20: Training loss 0.052 , Val loss 0.120 , Training accuracy 0.993 , Val accuracy 0.969\n",
            "best model saved\n",
            "Epoch 7/20: Training loss 0.043 , Val loss 0.103 , Training accuracy 0.989 , Val accuracy 0.974\n",
            "best model saved\n",
            "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 8/20: Training loss 0.036 , Val loss 0.109 , Training accuracy 0.994 , Val accuracy 0.976\n",
            "best model saved\n",
            "Epoch 9/20: Training loss 0.019 , Val loss 0.128 , Training accuracy 1.000 , Val accuracy 0.973\n",
            "Epoch 10/20: Training loss 0.011 , Val loss 0.117 , Training accuracy 0.999 , Val accuracy 0.969\n",
            "Epoch 11/20: Training loss 0.010 , Val loss 0.147 , Training accuracy 1.000 , Val accuracy 0.973\n",
            "checkpoint model saved\n",
            "Epoch    12: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 12/20: Training loss 0.007 , Val loss 0.157 , Training accuracy 1.000 , Val accuracy 0.977\n",
            "best model saved\n",
            "Epoch 13/20: Training loss 0.003 , Val loss 0.152 , Training accuracy 1.000 , Val accuracy 0.973\n",
            "Epoch 14/20: Training loss 0.003 , Val loss 0.158 , Training accuracy 1.000 , Val accuracy 0.973\n",
            "Epoch 15/20: Training loss 0.002 , Val loss 0.173 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch    16: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 16/20: Training loss 0.002 , Val loss 0.176 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 17/20: Training loss 0.003 , Val loss 0.192 , Training accuracy 1.000 , Val accuracy 0.974\n",
            "Epoch 18/20: Training loss 0.002 , Val loss 0.170 , Training accuracy 1.000 , Val accuracy 0.973\n",
            "Epoch 19/20: Training loss 0.002 , Val loss 0.218 , Training accuracy 1.000 , Val accuracy 0.974\n",
            "Epoch    20: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 20/20: Training loss 0.002 , Val loss 0.199 , Training accuracy 1.000 , Val accuracy 0.974\n",
            "Best training accuracy is 1.0\n",
            "Best val accuracy is 0.9766816143497757\n",
            "--------------------------------------------------------\n",
            "Training bs32 lr0.0005 \n",
            "Epoch 1/20: Training loss 0.498 , Val loss 0.159 , Training accuracy 0.942 , Val accuracy 0.961\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.148 , Val loss 0.161 , Training accuracy 0.967 , Val accuracy 0.961\n",
            "best model saved\n",
            "Epoch 3/20: Training loss 0.123 , Val loss 0.164 , Training accuracy 0.973 , Val accuracy 0.956\n",
            "Epoch 4/20: Training loss 0.104 , Val loss 0.261 , Training accuracy 0.967 , Val accuracy 0.935\n",
            "Epoch 5/20: Training loss 0.093 , Val loss 0.132 , Training accuracy 0.983 , Val accuracy 0.961\n",
            "best model saved\n",
            "Epoch 6/20: Training loss 0.082 , Val loss 0.134 , Training accuracy 0.981 , Val accuracy 0.967\n",
            "best model saved\n",
            "Epoch 7/20: Training loss 0.069 , Val loss 0.112 , Training accuracy 0.985 , Val accuracy 0.968\n",
            "best model saved\n",
            "Epoch     8: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 8/20: Training loss 0.061 , Val loss 0.123 , Training accuracy 0.987 , Val accuracy 0.974\n",
            "best model saved\n",
            "Epoch 9/20: Training loss 0.038 , Val loss 0.119 , Training accuracy 0.993 , Val accuracy 0.970\n",
            "Epoch 10/20: Training loss 0.026 , Val loss 0.125 , Training accuracy 0.997 , Val accuracy 0.964\n",
            "Epoch 11/20: Training loss 0.023 , Val loss 0.121 , Training accuracy 0.999 , Val accuracy 0.967\n",
            "checkpoint model saved\n",
            "Epoch    12: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 12/20: Training loss 0.020 , Val loss 0.119 , Training accuracy 0.998 , Val accuracy 0.969\n",
            "Epoch 13/20: Training loss 0.012 , Val loss 0.133 , Training accuracy 1.000 , Val accuracy 0.965\n",
            "Epoch 14/20: Training loss 0.012 , Val loss 0.184 , Training accuracy 0.997 , Val accuracy 0.957\n",
            "Epoch 15/20: Training loss 0.010 , Val loss 0.141 , Training accuracy 0.999 , Val accuracy 0.966\n",
            "Epoch    16: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 16/20: Training loss 0.011 , Val loss 0.181 , Training accuracy 1.000 , Val accuracy 0.966\n",
            "Epoch 17/20: Training loss 0.009 , Val loss 0.152 , Training accuracy 1.000 , Val accuracy 0.970\n",
            "Epoch 18/20: Training loss 0.006 , Val loss 0.202 , Training accuracy 0.999 , Val accuracy 0.961\n",
            "Epoch 19/20: Training loss 0.006 , Val loss 0.173 , Training accuracy 1.000 , Val accuracy 0.968\n",
            "Epoch    20: reducing learning rate of group 0 to 3.1250e-05.\n",
            "Epoch 20/20: Training loss 0.005 , Val loss 0.159 , Training accuracy 1.000 , Val accuracy 0.968\n",
            "Best training accuracy is 1.0\n",
            "Best val accuracy is 0.9739910313901345\n",
            "--------------------------------------------------------\n",
            "Training bs32 lr0.0001 \n",
            "Epoch 1/20: Training loss 0.687 , Val loss 0.718 , Training accuracy 0.524 , Val accuracy 0.147\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.577 , Val loss 0.425 , Training accuracy 0.933 , Val accuracy 0.940\n",
            "best model saved\n",
            "Epoch 3/20: Training loss 0.318 , Val loss 0.336 , Training accuracy 0.949 , Val accuracy 0.948\n",
            "best model saved\n",
            "Epoch 4/20: Training loss 0.205 , Val loss 0.334 , Training accuracy 0.947 , Val accuracy 0.906\n",
            "Epoch     5: reducing learning rate of group 0 to 5.0000e-05.\n",
            "Epoch 5/20: Training loss 0.163 , Val loss 0.137 , Training accuracy 0.945 , Val accuracy 0.966\n",
            "best model saved\n",
            "Epoch 6/20: Training loss 0.135 , Val loss 0.155 , Training accuracy 0.966 , Val accuracy 0.954\n",
            "Epoch 7/20: Training loss 0.129 , Val loss 0.161 , Training accuracy 0.968 , Val accuracy 0.956\n",
            "Epoch 8/20: Training loss 0.130 , Val loss 0.162 , Training accuracy 0.968 , Val accuracy 0.959\n",
            "Epoch     9: reducing learning rate of group 0 to 2.5000e-05.\n",
            "Epoch 9/20: Training loss 0.118 , Val loss 0.146 , Training accuracy 0.969 , Val accuracy 0.961\n",
            "Epoch 10/20: Training loss 0.114 , Val loss 0.136 , Training accuracy 0.969 , Val accuracy 0.961\n",
            "Epoch 11/20: Training loss 0.108 , Val loss 0.128 , Training accuracy 0.970 , Val accuracy 0.966\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 12/20: Training loss 0.105 , Val loss 0.132 , Training accuracy 0.972 , Val accuracy 0.964\n",
            "Epoch    13: reducing learning rate of group 0 to 1.2500e-05.\n",
            "Epoch 13/20: Training loss 0.108 , Val loss 0.136 , Training accuracy 0.973 , Val accuracy 0.964\n",
            "Epoch 14/20: Training loss 0.107 , Val loss 0.140 , Training accuracy 0.973 , Val accuracy 0.963\n",
            "Epoch 15/20: Training loss 0.100 , Val loss 0.149 , Training accuracy 0.972 , Val accuracy 0.954\n",
            "Epoch 16/20: Training loss 0.103 , Val loss 0.166 , Training accuracy 0.970 , Val accuracy 0.953\n",
            "Epoch    17: reducing learning rate of group 0 to 6.2500e-06.\n",
            "Epoch 17/20: Training loss 0.101 , Val loss 0.139 , Training accuracy 0.972 , Val accuracy 0.967\n",
            "best model saved\n",
            "Epoch 18/20: Training loss 0.097 , Val loss 0.178 , Training accuracy 0.969 , Val accuracy 0.951\n",
            "Epoch 19/20: Training loss 0.098 , Val loss 0.135 , Training accuracy 0.972 , Val accuracy 0.965\n",
            "Epoch 20/20: Training loss 0.099 , Val loss 0.140 , Training accuracy 0.974 , Val accuracy 0.965\n",
            "Best training accuracy is 0.9739574748640185\n",
            "Best val accuracy is 0.9668161434977578\n",
            "--------------------------------------------------------\n",
            "Training bs64 lr0.001 \n",
            "Epoch 1/20: Training loss 0.649 , Val loss 0.588 , Training accuracy 0.923 , Val accuracy 0.877\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.222 , Val loss 0.215 , Training accuracy 0.959 , Val accuracy 0.944\n",
            "best model saved\n",
            "Epoch 3/20: Training loss 0.142 , Val loss 0.157 , Training accuracy 0.966 , Val accuracy 0.964\n",
            "best model saved\n",
            "Epoch 4/20: Training loss 0.118 , Val loss 0.193 , Training accuracy 0.972 , Val accuracy 0.955\n",
            "Epoch     5: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 5/20: Training loss 0.106 , Val loss 0.146 , Training accuracy 0.977 , Val accuracy 0.962\n",
            "Epoch 6/20: Training loss 0.083 , Val loss 0.128 , Training accuracy 0.981 , Val accuracy 0.968\n",
            "best model saved\n",
            "Epoch 7/20: Training loss 0.062 , Val loss 0.124 , Training accuracy 0.987 , Val accuracy 0.964\n",
            "Epoch 8/20: Training loss 0.069 , Val loss 0.115 , Training accuracy 0.987 , Val accuracy 0.970\n",
            "best model saved\n",
            "Epoch     9: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 9/20: Training loss 0.057 , Val loss 0.146 , Training accuracy 0.983 , Val accuracy 0.962\n",
            "Epoch 10/20: Training loss 0.049 , Val loss 0.144 , Training accuracy 0.990 , Val accuracy 0.968\n",
            "Epoch 11/20: Training loss 0.043 , Val loss 0.131 , Training accuracy 0.992 , Val accuracy 0.968\n",
            "checkpoint model saved\n",
            "Epoch 12/20: Training loss 0.037 , Val loss 0.111 , Training accuracy 0.989 , Val accuracy 0.971\n",
            "best model saved\n",
            "Epoch    13: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 13/20: Training loss 0.030 , Val loss 0.142 , Training accuracy 0.995 , Val accuracy 0.966\n",
            "Epoch 14/20: Training loss 0.023 , Val loss 0.119 , Training accuracy 0.996 , Val accuracy 0.970\n",
            "Epoch 15/20: Training loss 0.021 , Val loss 0.138 , Training accuracy 0.996 , Val accuracy 0.970\n",
            "Epoch 16/20: Training loss 0.019 , Val loss 0.120 , Training accuracy 0.998 , Val accuracy 0.970\n",
            "Epoch    17: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 17/20: Training loss 0.016 , Val loss 0.135 , Training accuracy 0.999 , Val accuracy 0.968\n",
            "Epoch 18/20: Training loss 0.014 , Val loss 0.141 , Training accuracy 0.998 , Val accuracy 0.970\n",
            "Epoch 19/20: Training loss 0.013 , Val loss 0.139 , Training accuracy 0.999 , Val accuracy 0.968\n",
            "Epoch 20/20: Training loss 0.011 , Val loss 0.130 , Training accuracy 0.999 , Val accuracy 0.969\n",
            "Best training accuracy is 0.9993406955661777\n",
            "Best val accuracy is 0.9713004484304932\n",
            "--------------------------------------------------------\n",
            "Training bs64 lr0.0005 \n",
            "Epoch 1/20: Training loss 0.669 , Val loss 0.728 , Training accuracy 0.525 , Val accuracy 0.149\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.360 , Val loss 0.230 , Training accuracy 0.956 , Val accuracy 0.955\n",
            "best model saved\n",
            "Epoch 3/20: Training loss 0.164 , Val loss 0.165 , Training accuracy 0.961 , Val accuracy 0.962\n",
            "best model saved\n",
            "Epoch 4/20: Training loss 0.135 , Val loss 0.154 , Training accuracy 0.967 , Val accuracy 0.965\n",
            "best model saved\n",
            "Epoch     5: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 5/20: Training loss 0.125 , Val loss 0.143 , Training accuracy 0.970 , Val accuracy 0.964\n",
            "Epoch 6/20: Training loss 0.110 , Val loss 0.144 , Training accuracy 0.971 , Val accuracy 0.965\n",
            "best model saved\n",
            "Epoch 7/20: Training loss 0.103 , Val loss 0.142 , Training accuracy 0.974 , Val accuracy 0.965\n",
            "best model saved\n",
            "Epoch 8/20: Training loss 0.104 , Val loss 0.146 , Training accuracy 0.974 , Val accuracy 0.966\n",
            "best model saved\n",
            "Epoch     9: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 9/20: Training loss 0.096 , Val loss 0.182 , Training accuracy 0.971 , Val accuracy 0.952\n",
            "Epoch 10/20: Training loss 0.089 , Val loss 0.154 , Training accuracy 0.978 , Val accuracy 0.964\n",
            "Epoch 11/20: Training loss 0.088 , Val loss 0.143 , Training accuracy 0.979 , Val accuracy 0.965\n",
            "checkpoint model saved\n",
            "Epoch 12/20: Training loss 0.087 , Val loss 0.112 , Training accuracy 0.973 , Val accuracy 0.970\n",
            "best model saved\n",
            "Epoch    13: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 13/20: Training loss 0.077 , Val loss 0.136 , Training accuracy 0.980 , Val accuracy 0.964\n",
            "Epoch 14/20: Training loss 0.071 , Val loss 0.132 , Training accuracy 0.980 , Val accuracy 0.965\n",
            "Epoch 15/20: Training loss 0.072 , Val loss 0.140 , Training accuracy 0.981 , Val accuracy 0.966\n",
            "Epoch 16/20: Training loss 0.071 , Val loss 0.119 , Training accuracy 0.980 , Val accuracy 0.969\n",
            "Epoch    17: reducing learning rate of group 0 to 3.1250e-05.\n",
            "Epoch 17/20: Training loss 0.065 , Val loss 0.140 , Training accuracy 0.985 , Val accuracy 0.968\n",
            "Epoch 18/20: Training loss 0.063 , Val loss 0.125 , Training accuracy 0.983 , Val accuracy 0.967\n",
            "Epoch 19/20: Training loss 0.060 , Val loss 0.125 , Training accuracy 0.984 , Val accuracy 0.967\n",
            "Epoch 20/20: Training loss 0.062 , Val loss 0.134 , Training accuracy 0.983 , Val accuracy 0.967\n",
            "Best training accuracy is 0.9845063458051755\n",
            "Best val accuracy is 0.9704035874439462\n",
            "--------------------------------------------------------\n",
            "Training bs64 lr0.0001 \n",
            "Epoch 1/20: Training loss 0.692 , Val loss 0.709 , Training accuracy 0.524 , Val accuracy 0.147\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.679 , Val loss 0.711 , Training accuracy 0.524 , Val accuracy 0.147\n",
            "best model saved\n",
            "Epoch 3/20: Training loss 0.634 , Val loss 0.637 , Training accuracy 0.781 , Val accuracy 0.858\n",
            "best model saved\n",
            "Epoch 4/20: Training loss 0.512 , Val loss 0.479 , Training accuracy 0.945 , Val accuracy 0.934\n",
            "best model saved\n",
            "Epoch     5: reducing learning rate of group 0 to 5.0000e-05.\n",
            "Epoch 5/20: Training loss 0.323 , Val loss 0.334 , Training accuracy 0.954 , Val accuracy 0.942\n",
            "best model saved\n",
            "Epoch 6/20: Training loss 0.236 , Val loss 0.261 , Training accuracy 0.949 , Val accuracy 0.949\n",
            "best model saved\n",
            "Epoch 7/20: Training loss 0.222 , Val loss 0.236 , Training accuracy 0.958 , Val accuracy 0.947\n",
            "Epoch 8/20: Training loss 0.234 , Val loss 0.215 , Training accuracy 0.956 , Val accuracy 0.953\n",
            "best model saved\n",
            "Epoch     9: reducing learning rate of group 0 to 2.5000e-05.\n",
            "Epoch 9/20: Training loss 0.184 , Val loss 0.606 , Training accuracy 0.827 , Val accuracy 0.697\n",
            "Epoch 10/20: Training loss 0.183 , Val loss 0.206 , Training accuracy 0.958 , Val accuracy 0.950\n",
            "Epoch 11/20: Training loss 0.158 , Val loss 0.181 , Training accuracy 0.960 , Val accuracy 0.952\n",
            "checkpoint model saved\n",
            "Epoch 12/20: Training loss 0.151 , Val loss 0.150 , Training accuracy 0.952 , Val accuracy 0.960\n",
            "best model saved\n",
            "Epoch    13: reducing learning rate of group 0 to 1.2500e-05.\n",
            "Epoch 13/20: Training loss 0.146 , Val loss 0.165 , Training accuracy 0.962 , Val accuracy 0.955\n",
            "Epoch 14/20: Training loss 0.142 , Val loss 0.162 , Training accuracy 0.961 , Val accuracy 0.953\n",
            "Epoch 15/20: Training loss 0.141 , Val loss 0.183 , Training accuracy 0.960 , Val accuracy 0.951\n",
            "Epoch 16/20: Training loss 0.138 , Val loss 0.151 , Training accuracy 0.963 , Val accuracy 0.958\n",
            "Epoch    17: reducing learning rate of group 0 to 6.2500e-06.\n",
            "Epoch 17/20: Training loss 0.140 , Val loss 0.164 , Training accuracy 0.961 , Val accuracy 0.953\n",
            "Epoch 18/20: Training loss 0.137 , Val loss 0.144 , Training accuracy 0.964 , Val accuracy 0.962\n",
            "best model saved\n",
            "Epoch 19/20: Training loss 0.133 , Val loss 0.151 , Training accuracy 0.963 , Val accuracy 0.956\n",
            "Epoch 20/20: Training loss 0.137 , Val loss 0.158 , Training accuracy 0.964 , Val accuracy 0.957\n",
            "Best training accuracy is 0.9637382561397725\n",
            "Best val accuracy is 0.9623318385650225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best val accuracy at 32 bs and 0.001 lr\n",
        "for hidden_layers in [64,128,256]:\n",
        "  for dropout in [0.1,0.25,0.5]:\n",
        "    print (\"--------------------------------------------------------\")\n",
        "    print (f\"Training hidden {hidden_layers} dropout {dropout} \")\n",
        "    SimpleRNN = RNN(hidden_layers,dropout)\n",
        "    train_rnn(SimpleRNN, train_data, val_data, num_epochs= 20, batch_size = 32, learning_rate=0.001, hidden_layers = hidden_layers, dropout = dropout, plot = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl2qAmS8tanH",
        "outputId": "2ccf5eae-e68b-48de-a15c-90eacf41c784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------\n",
            "Training hidden 64 dropout 0.1 \n",
            "Epoch 1/20: Training loss 0.454 , Val loss 0.122 , Training accuracy 0.952 , Val accuracy 0.964\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.129 , Val loss 0.145 , Training accuracy 0.969 , Val accuracy 0.961\n",
            "Epoch 3/20: Training loss 0.106 , Val loss 0.154 , Training accuracy 0.972 , Val accuracy 0.958\n",
            "Epoch 4/20: Training loss 0.085 , Val loss 0.236 , Training accuracy 0.970 , Val accuracy 0.935\n",
            "Epoch 5/20: Training loss 0.072 , Val loss 0.120 , Training accuracy 0.985 , Val accuracy 0.966\n",
            "best model saved\n",
            "Epoch 6/20: Training loss 0.051 , Val loss 0.114 , Training accuracy 0.991 , Val accuracy 0.968\n",
            "best model saved\n",
            "Epoch 7/20: Training loss 0.043 , Val loss 0.105 , Training accuracy 0.945 , Val accuracy 0.968\n",
            "best model saved\n",
            "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 8/20: Training loss 0.052 , Val loss 0.104 , Training accuracy 0.991 , Val accuracy 0.971\n",
            "best model saved\n",
            "Epoch 9/20: Training loss 0.022 , Val loss 0.104 , Training accuracy 0.998 , Val accuracy 0.970\n",
            "Epoch 10/20: Training loss 0.016 , Val loss 0.119 , Training accuracy 0.999 , Val accuracy 0.966\n",
            "Epoch 11/20: Training loss 0.012 , Val loss 0.121 , Training accuracy 0.999 , Val accuracy 0.968\n",
            "checkpoint model saved\n",
            "Epoch    12: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 12/20: Training loss 0.009 , Val loss 0.128 , Training accuracy 0.999 , Val accuracy 0.970\n",
            "Epoch 13/20: Training loss 0.005 , Val loss 0.124 , Training accuracy 1.000 , Val accuracy 0.969\n",
            "Epoch 14/20: Training loss 0.004 , Val loss 0.138 , Training accuracy 1.000 , Val accuracy 0.968\n",
            "Epoch 15/20: Training loss 0.004 , Val loss 0.137 , Training accuracy 1.000 , Val accuracy 0.972\n",
            "best model saved\n",
            "Epoch    16: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 16/20: Training loss 0.003 , Val loss 0.136 , Training accuracy 1.000 , Val accuracy 0.971\n",
            "Epoch 17/20: Training loss 0.003 , Val loss 0.135 , Training accuracy 1.000 , Val accuracy 0.968\n",
            "Epoch 18/20: Training loss 0.002 , Val loss 0.152 , Training accuracy 1.000 , Val accuracy 0.966\n",
            "Epoch 19/20: Training loss 0.002 , Val loss 0.152 , Training accuracy 1.000 , Val accuracy 0.971\n",
            "Epoch    20: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 20/20: Training loss 0.002 , Val loss 0.154 , Training accuracy 1.000 , Val accuracy 0.968\n",
            "Best training accuracy is 1.0\n",
            "Best val accuracy is 0.9721973094170404\n",
            "--------------------------------------------------------\n",
            "Training hidden 64 dropout 0.25 \n",
            "Epoch 1/20: Training loss 0.459 , Val loss 0.123 , Training accuracy 0.949 , Val accuracy 0.965\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.130 , Val loss 0.168 , Training accuracy 0.970 , Val accuracy 0.957\n",
            "Epoch 3/20: Training loss 0.105 , Val loss 0.146 , Training accuracy 0.975 , Val accuracy 0.956\n",
            "Epoch 4/20: Training loss 0.080 , Val loss 0.245 , Training accuracy 0.970 , Val accuracy 0.934\n",
            "Epoch 5/20: Training loss 0.071 , Val loss 0.126 , Training accuracy 0.986 , Val accuracy 0.966\n",
            "best model saved\n",
            "Epoch 6/20: Training loss 0.051 , Val loss 0.110 , Training accuracy 0.990 , Val accuracy 0.967\n",
            "best model saved\n",
            "Epoch 7/20: Training loss 0.037 , Val loss 0.116 , Training accuracy 0.991 , Val accuracy 0.974\n",
            "best model saved\n",
            "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 8/20: Training loss 0.034 , Val loss 0.113 , Training accuracy 0.994 , Val accuracy 0.977\n",
            "best model saved\n",
            "Epoch 9/20: Training loss 0.013 , Val loss 0.113 , Training accuracy 1.000 , Val accuracy 0.971\n",
            "Epoch 10/20: Training loss 0.009 , Val loss 0.134 , Training accuracy 0.999 , Val accuracy 0.967\n",
            "Epoch 11/20: Training loss 0.008 , Val loss 0.124 , Training accuracy 1.000 , Val accuracy 0.969\n",
            "checkpoint model saved\n",
            "Epoch    12: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 12/20: Training loss 0.006 , Val loss 0.152 , Training accuracy 0.999 , Val accuracy 0.974\n",
            "Epoch 13/20: Training loss 0.004 , Val loss 0.140 , Training accuracy 1.000 , Val accuracy 0.967\n",
            "Epoch 14/20: Training loss 0.003 , Val loss 0.154 , Training accuracy 1.000 , Val accuracy 0.970\n",
            "Epoch 15/20: Training loss 0.003 , Val loss 0.155 , Training accuracy 1.000 , Val accuracy 0.969\n",
            "Epoch    16: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 16/20: Training loss 0.004 , Val loss 0.151 , Training accuracy 1.000 , Val accuracy 0.970\n",
            "Epoch 17/20: Training loss 0.003 , Val loss 0.161 , Training accuracy 1.000 , Val accuracy 0.970\n",
            "Epoch 18/20: Training loss 0.002 , Val loss 0.189 , Training accuracy 1.000 , Val accuracy 0.966\n",
            "Epoch 19/20: Training loss 0.002 , Val loss 0.162 , Training accuracy 1.000 , Val accuracy 0.970\n",
            "Epoch    20: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 20/20: Training loss 0.002 , Val loss 0.169 , Training accuracy 1.000 , Val accuracy 0.968\n",
            "Best training accuracy is 1.0\n",
            "Best val accuracy is 0.9766816143497757\n",
            "--------------------------------------------------------\n",
            "Training hidden 64 dropout 0.5 \n",
            "Epoch 1/20: Training loss 0.486 , Val loss 0.161 , Training accuracy 0.955 , Val accuracy 0.961\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.144 , Val loss 0.161 , Training accuracy 0.965 , Val accuracy 0.963\n",
            "best model saved\n",
            "Epoch 3/20: Training loss 0.122 , Val loss 0.168 , Training accuracy 0.972 , Val accuracy 0.957\n",
            "Epoch 4/20: Training loss 0.097 , Val loss 0.247 , Training accuracy 0.971 , Val accuracy 0.935\n",
            "Epoch 5/20: Training loss 0.083 , Val loss 0.120 , Training accuracy 0.986 , Val accuracy 0.968\n",
            "best model saved\n",
            "Epoch 6/20: Training loss 0.066 , Val loss 0.115 , Training accuracy 0.985 , Val accuracy 0.969\n",
            "best model saved\n",
            "Epoch 7/20: Training loss 0.056 , Val loss 0.115 , Training accuracy 0.987 , Val accuracy 0.973\n",
            "best model saved\n",
            "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 8/20: Training loss 0.046 , Val loss 0.122 , Training accuracy 0.990 , Val accuracy 0.975\n",
            "best model saved\n",
            "Epoch 9/20: Training loss 0.025 , Val loss 0.123 , Training accuracy 0.998 , Val accuracy 0.972\n",
            "Epoch 10/20: Training loss 0.019 , Val loss 0.116 , Training accuracy 0.995 , Val accuracy 0.974\n",
            "Epoch 11/20: Training loss 0.014 , Val loss 0.170 , Training accuracy 0.999 , Val accuracy 0.968\n",
            "checkpoint model saved\n",
            "Epoch    12: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 12/20: Training loss 0.014 , Val loss 0.160 , Training accuracy 0.999 , Val accuracy 0.971\n",
            "Epoch 13/20: Training loss 0.009 , Val loss 0.166 , Training accuracy 1.000 , Val accuracy 0.969\n",
            "Epoch 14/20: Training loss 0.008 , Val loss 0.180 , Training accuracy 1.000 , Val accuracy 0.971\n",
            "Epoch 15/20: Training loss 0.007 , Val loss 0.171 , Training accuracy 1.000 , Val accuracy 0.967\n",
            "Epoch    16: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 16/20: Training loss 0.009 , Val loss 0.187 , Training accuracy 1.000 , Val accuracy 0.970\n",
            "Epoch 17/20: Training loss 0.005 , Val loss 0.187 , Training accuracy 1.000 , Val accuracy 0.970\n",
            "Epoch 18/20: Training loss 0.004 , Val loss 0.205 , Training accuracy 1.000 , Val accuracy 0.969\n",
            "Epoch 19/20: Training loss 0.004 , Val loss 0.192 , Training accuracy 1.000 , Val accuracy 0.970\n",
            "Epoch    20: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 20/20: Training loss 0.004 , Val loss 0.189 , Training accuracy 1.000 , Val accuracy 0.970\n",
            "Best training accuracy is 1.0\n",
            "Best val accuracy is 0.9748878923766816\n",
            "--------------------------------------------------------\n",
            "Training hidden 128 dropout 0.1 \n",
            "Epoch 1/20: Training loss 0.378 , Val loss 0.116 , Training accuracy 0.955 , Val accuracy 0.967\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.115 , Val loss 0.170 , Training accuracy 0.969 , Val accuracy 0.952\n",
            "Epoch 3/20: Training loss 0.087 , Val loss 0.111 , Training accuracy 0.980 , Val accuracy 0.970\n",
            "best model saved\n",
            "Epoch 4/20: Training loss 0.061 , Val loss 0.182 , Training accuracy 0.981 , Val accuracy 0.948\n",
            "Epoch 5/20: Training loss 0.036 , Val loss 0.098 , Training accuracy 0.996 , Val accuracy 0.973\n",
            "best model saved\n",
            "Epoch 6/20: Training loss 0.021 , Val loss 0.113 , Training accuracy 0.997 , Val accuracy 0.973\n",
            "best model saved\n",
            "Epoch 7/20: Training loss 0.020 , Val loss 0.101 , Training accuracy 0.992 , Val accuracy 0.969\n",
            "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 8/20: Training loss 0.034 , Val loss 0.097 , Training accuracy 0.995 , Val accuracy 0.978\n",
            "best model saved\n",
            "Epoch 9/20: Training loss 0.008 , Val loss 0.083 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 10/20: Training loss 0.003 , Val loss 0.096 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 11/20: Training loss 0.002 , Val loss 0.098 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "checkpoint model saved\n",
            "Epoch    12: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 12/20: Training loss 0.002 , Val loss 0.099 , Training accuracy 1.000 , Val accuracy 0.974\n",
            "Epoch 13/20: Training loss 0.001 , Val loss 0.109 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 14/20: Training loss 0.001 , Val loss 0.109 , Training accuracy 1.000 , Val accuracy 0.974\n",
            "Epoch 15/20: Training loss 0.001 , Val loss 0.112 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch    16: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 16/20: Training loss 0.001 , Val loss 0.112 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 17/20: Training loss 0.001 , Val loss 0.117 , Training accuracy 1.000 , Val accuracy 0.974\n",
            "Epoch 18/20: Training loss 0.001 , Val loss 0.121 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 19/20: Training loss 0.001 , Val loss 0.116 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch    20: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 20/20: Training loss 0.000 , Val loss 0.120 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Best training accuracy is 1.0\n",
            "Best val accuracy is 0.9775784753363229\n",
            "--------------------------------------------------------\n",
            "Training hidden 128 dropout 0.25 \n",
            "Epoch 1/20: Training loss 0.367 , Val loss 0.116 , Training accuracy 0.955 , Val accuracy 0.965\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.119 , Val loss 0.152 , Training accuracy 0.972 , Val accuracy 0.960\n",
            "Epoch 3/20: Training loss 0.093 , Val loss 0.111 , Training accuracy 0.976 , Val accuracy 0.968\n",
            "best model saved\n",
            "Epoch 4/20: Training loss 0.071 , Val loss 0.167 , Training accuracy 0.984 , Val accuracy 0.958\n",
            "Epoch 5/20: Training loss 0.050 , Val loss 0.102 , Training accuracy 0.993 , Val accuracy 0.970\n",
            "best model saved\n",
            "Epoch 6/20: Training loss 0.035 , Val loss 0.089 , Training accuracy 0.998 , Val accuracy 0.973\n",
            "best model saved\n",
            "Epoch 7/20: Training loss 0.026 , Val loss 0.117 , Training accuracy 0.970 , Val accuracy 0.978\n",
            "best model saved\n",
            "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 8/20: Training loss 0.025 , Val loss 0.110 , Training accuracy 0.998 , Val accuracy 0.975\n",
            "Epoch 9/20: Training loss 0.006 , Val loss 0.106 , Training accuracy 1.000 , Val accuracy 0.974\n",
            "Epoch 10/20: Training loss 0.004 , Val loss 0.122 , Training accuracy 1.000 , Val accuracy 0.972\n",
            "Epoch 11/20: Training loss 0.003 , Val loss 0.114 , Training accuracy 1.000 , Val accuracy 0.973\n",
            "checkpoint model saved\n",
            "Epoch    12: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 12/20: Training loss 0.002 , Val loss 0.120 , Training accuracy 1.000 , Val accuracy 0.977\n",
            "Epoch 13/20: Training loss 0.001 , Val loss 0.120 , Training accuracy 1.000 , Val accuracy 0.973\n",
            "Epoch 14/20: Training loss 0.001 , Val loss 0.133 , Training accuracy 1.000 , Val accuracy 0.973\n",
            "Epoch 15/20: Training loss 0.001 , Val loss 0.134 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch    16: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 16/20: Training loss 0.001 , Val loss 0.141 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 17/20: Training loss 0.001 , Val loss 0.141 , Training accuracy 1.000 , Val accuracy 0.974\n",
            "Epoch 18/20: Training loss 0.001 , Val loss 0.151 , Training accuracy 1.000 , Val accuracy 0.973\n",
            "Epoch 19/20: Training loss 0.001 , Val loss 0.146 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch    20: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 20/20: Training loss 0.001 , Val loss 0.148 , Training accuracy 1.000 , Val accuracy 0.974\n",
            "Best training accuracy is 1.0\n",
            "Best val accuracy is 0.9775784753363229\n",
            "--------------------------------------------------------\n",
            "Training hidden 128 dropout 0.5 \n",
            "Epoch 1/20: Training loss 0.428 , Val loss 0.128 , Training accuracy 0.946 , Val accuracy 0.961\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.134 , Val loss 0.193 , Training accuracy 0.970 , Val accuracy 0.956\n",
            "Epoch 3/20: Training loss 0.118 , Val loss 0.153 , Training accuracy 0.980 , Val accuracy 0.962\n",
            "best model saved\n",
            "Epoch 4/20: Training loss 0.079 , Val loss 0.207 , Training accuracy 0.976 , Val accuracy 0.948\n",
            "Epoch 5/20: Training loss 0.068 , Val loss 0.126 , Training accuracy 0.990 , Val accuracy 0.964\n",
            "best model saved\n",
            "Epoch 6/20: Training loss 0.053 , Val loss 0.127 , Training accuracy 0.995 , Val accuracy 0.967\n",
            "best model saved\n",
            "Epoch 7/20: Training loss 0.047 , Val loss 0.120 , Training accuracy 0.969 , Val accuracy 0.973\n",
            "best model saved\n",
            "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 8/20: Training loss 0.056 , Val loss 0.114 , Training accuracy 0.993 , Val accuracy 0.977\n",
            "best model saved\n",
            "Epoch 9/20: Training loss 0.028 , Val loss 0.142 , Training accuracy 0.995 , Val accuracy 0.970\n",
            "Epoch 10/20: Training loss 0.019 , Val loss 0.116 , Training accuracy 0.999 , Val accuracy 0.970\n",
            "Epoch 11/20: Training loss 0.013 , Val loss 0.127 , Training accuracy 0.999 , Val accuracy 0.970\n",
            "checkpoint model saved\n",
            "Epoch    12: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 12/20: Training loss 0.010 , Val loss 0.125 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 13/20: Training loss 0.006 , Val loss 0.162 , Training accuracy 1.000 , Val accuracy 0.970\n",
            "Epoch 14/20: Training loss 0.006 , Val loss 0.151 , Training accuracy 1.000 , Val accuracy 0.970\n",
            "Epoch 15/20: Training loss 0.005 , Val loss 0.148 , Training accuracy 1.000 , Val accuracy 0.970\n",
            "Epoch    16: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 16/20: Training loss 0.007 , Val loss 0.161 , Training accuracy 1.000 , Val accuracy 0.972\n",
            "Epoch 17/20: Training loss 0.003 , Val loss 0.171 , Training accuracy 1.000 , Val accuracy 0.974\n",
            "Epoch 18/20: Training loss 0.003 , Val loss 0.170 , Training accuracy 1.000 , Val accuracy 0.970\n",
            "Epoch 19/20: Training loss 0.003 , Val loss 0.172 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch    20: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 20/20: Training loss 0.002 , Val loss 0.178 , Training accuracy 1.000 , Val accuracy 0.973\n",
            "Best training accuracy is 1.0\n",
            "Best val accuracy is 0.9766816143497757\n",
            "--------------------------------------------------------\n",
            "Training hidden 256 dropout 0.1 \n",
            "Epoch 1/20: Training loss 0.363 , Val loss 0.114 , Training accuracy 0.957 , Val accuracy 0.965\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.110 , Val loss 0.235 , Training accuracy 0.967 , Val accuracy 0.931\n",
            "Epoch 3/20: Training loss 0.075 , Val loss 0.093 , Training accuracy 0.980 , Val accuracy 0.973\n",
            "best model saved\n",
            "Epoch 4/20: Training loss 0.051 , Val loss 0.192 , Training accuracy 0.985 , Val accuracy 0.946\n",
            "Epoch 5/20: Training loss 0.028 , Val loss 0.095 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "best model saved\n",
            "Epoch     6: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 6/20: Training loss 0.025 , Val loss 0.082 , Training accuracy 0.995 , Val accuracy 0.974\n",
            "Epoch 7/20: Training loss 0.015 , Val loss 0.084 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "best model saved\n",
            "Epoch 8/20: Training loss 0.004 , Val loss 0.092 , Training accuracy 1.000 , Val accuracy 0.977\n",
            "best model saved\n",
            "Epoch 9/20: Training loss 0.002 , Val loss 0.100 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch    10: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 10/20: Training loss 0.001 , Val loss 0.112 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch 11/20: Training loss 0.001 , Val loss 0.117 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "checkpoint model saved\n",
            "Epoch 12/20: Training loss 0.001 , Val loss 0.109 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch 13/20: Training loss 0.001 , Val loss 0.117 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch    14: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 14/20: Training loss 0.000 , Val loss 0.121 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch 15/20: Training loss 0.000 , Val loss 0.124 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch 16/20: Training loss 0.000 , Val loss 0.129 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch 17/20: Training loss 0.000 , Val loss 0.122 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch    18: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 18/20: Training loss 0.000 , Val loss 0.132 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 19/20: Training loss 0.000 , Val loss 0.128 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch 20/20: Training loss 0.000 , Val loss 0.127 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Best training accuracy is 1.0\n",
            "Best val accuracy is 0.9766816143497757\n",
            "--------------------------------------------------------\n",
            "Training hidden 256 dropout 0.25 \n",
            "Epoch 1/20: Training loss 0.387 , Val loss 0.105 , Training accuracy 0.956 , Val accuracy 0.969\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.111 , Val loss 0.189 , Training accuracy 0.969 , Val accuracy 0.943\n",
            "Epoch 3/20: Training loss 0.080 , Val loss 0.092 , Training accuracy 0.982 , Val accuracy 0.974\n",
            "best model saved\n",
            "Epoch 4/20: Training loss 0.057 , Val loss 0.193 , Training accuracy 0.981 , Val accuracy 0.944\n",
            "Epoch 5/20: Training loss 0.038 , Val loss 0.098 , Training accuracy 0.998 , Val accuracy 0.975\n",
            "best model saved\n",
            "Epoch     6: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 6/20: Training loss 0.022 , Val loss 0.094 , Training accuracy 0.994 , Val accuracy 0.976\n",
            "best model saved\n",
            "Epoch 7/20: Training loss 0.011 , Val loss 0.104 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "best model saved\n",
            "Epoch 8/20: Training loss 0.003 , Val loss 0.123 , Training accuracy 1.000 , Val accuracy 0.978\n",
            "best model saved\n",
            "Epoch 9/20: Training loss 0.002 , Val loss 0.131 , Training accuracy 1.000 , Val accuracy 0.978\n",
            "best model saved\n",
            "Epoch    10: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 10/20: Training loss 0.002 , Val loss 0.126 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 11/20: Training loss 0.001 , Val loss 0.145 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "checkpoint model saved\n",
            "Epoch 12/20: Training loss 0.001 , Val loss 0.138 , Training accuracy 1.000 , Val accuracy 0.978\n",
            "best model saved\n",
            "Epoch 13/20: Training loss 0.001 , Val loss 0.140 , Training accuracy 1.000 , Val accuracy 0.978\n",
            "best model saved\n",
            "Epoch    14: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 14/20: Training loss 0.000 , Val loss 0.146 , Training accuracy 1.000 , Val accuracy 0.977\n",
            "Epoch 15/20: Training loss 0.000 , Val loss 0.151 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch 16/20: Training loss 0.000 , Val loss 0.143 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 17/20: Training loss 0.000 , Val loss 0.147 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch    18: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 18/20: Training loss 0.000 , Val loss 0.156 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 19/20: Training loss 0.000 , Val loss 0.148 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch 20/20: Training loss 0.000 , Val loss 0.164 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Best training accuracy is 1.0\n",
            "Best val accuracy is 0.9775784753363229\n",
            "--------------------------------------------------------\n",
            "Training hidden 256 dropout 0.5 \n",
            "Epoch 1/20: Training loss 0.409 , Val loss 0.127 , Training accuracy 0.953 , Val accuracy 0.965\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 2/20: Training loss 0.129 , Val loss 0.221 , Training accuracy 0.967 , Val accuracy 0.936\n",
            "Epoch 3/20: Training loss 0.119 , Val loss 0.131 , Training accuracy 0.977 , Val accuracy 0.968\n",
            "best model saved\n",
            "Epoch 4/20: Training loss 0.089 , Val loss 0.234 , Training accuracy 0.975 , Val accuracy 0.942\n",
            "Epoch 5/20: Training loss 0.065 , Val loss 0.130 , Training accuracy 0.989 , Val accuracy 0.963\n",
            "Epoch     6: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 6/20: Training loss 0.050 , Val loss 0.120 , Training accuracy 0.995 , Val accuracy 0.970\n",
            "best model saved\n",
            "Epoch 7/20: Training loss 0.026 , Val loss 0.108 , Training accuracy 0.998 , Val accuracy 0.975\n",
            "best model saved\n",
            "Epoch 8/20: Training loss 0.015 , Val loss 0.138 , Training accuracy 0.998 , Val accuracy 0.976\n",
            "best model saved\n",
            "Epoch 9/20: Training loss 0.011 , Val loss 0.127 , Training accuracy 1.000 , Val accuracy 0.977\n",
            "best model saved\n",
            "Epoch    10: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 10/20: Training loss 0.009 , Val loss 0.144 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch 11/20: Training loss 0.005 , Val loss 0.134 , Training accuracy 1.000 , Val accuracy 0.977\n",
            "best model saved\n",
            "checkpoint model saved\n",
            "Epoch 12/20: Training loss 0.003 , Val loss 0.160 , Training accuracy 1.000 , Val accuracy 0.978\n",
            "best model saved\n",
            "Epoch 13/20: Training loss 0.004 , Val loss 0.135 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch    14: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 14/20: Training loss 0.002 , Val loss 0.172 , Training accuracy 1.000 , Val accuracy 0.977\n",
            "Epoch 15/20: Training loss 0.001 , Val loss 0.169 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Epoch 16/20: Training loss 0.001 , Val loss 0.173 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 17/20: Training loss 0.001 , Val loss 0.179 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch    18: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 18/20: Training loss 0.001 , Val loss 0.208 , Training accuracy 1.000 , Val accuracy 0.975\n",
            "Epoch 19/20: Training loss 0.001 , Val loss 0.171 , Training accuracy 1.000 , Val accuracy 0.974\n",
            "Epoch 20/20: Training loss 0.001 , Val loss 0.184 , Training accuracy 1.000 , Val accuracy 0.976\n",
            "Best training accuracy is 1.0\n",
            "Best val accuracy is 0.9775784753363229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7DY56rKa91I"
      },
      "source": [
        "### Part (d) [2 pt]\n",
        "\n",
        "Before we deploy a machine learning model, we usually want to have a better understanding\n",
        "of how our model performs beyond its validation accuracy. An important metric to track is\n",
        "*how well our model performs in certain subsets of the data*.\n",
        "\n",
        "In particular, what is the model's error rate amongst data with negative labels?\n",
        "This is called the **false positive rate**.\n",
        "\n",
        "What about the model's error rate amongst data with positive labels?\n",
        "This is called the **false negative rate**.\n",
        "\n",
        "Report your final model's false positive and false negative rate across the\n",
        "validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ggbQSdba91J",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d077ca15-2ff0-4c0e-9869-b910e394eb21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False positive rate on val dataset is 0.946%\n",
            "False negative rate on val dataset is 10.976%\n"
          ]
        }
      ],
      "source": [
        "# Create a Dataset of only spam validation examples\n",
        "valid_spam = torchtext.legacy.data.Dataset(\n",
        "    [e for e in val_data.examples if e.label == 1],\n",
        "    val_data.fields)\n",
        "# Create a Dataset of only non-spam validation examples\n",
        "valid_nospam = torchtext.legacy.data.Dataset(\n",
        "    [e for e in val_data.examples if e.label == 0],\n",
        "    val_data.fields)\n",
        "\n",
        "val_spam= torchtext.legacy.data.BucketIterator(\n",
        "                    valid_spam,\n",
        "                    batch_size=32,\n",
        "                    sort_key=lambda x: len(x.sms), \n",
        "                    sort_within_batch=True,        \n",
        "                    repeat=False)                  \n",
        "\n",
        "val_nospam = torchtext.legacy.data.BucketIterator(\n",
        "                    valid_nospam,\n",
        "                    batch_size=32,\n",
        "                    sort_key=lambda x: len(x.sms), \n",
        "                    sort_within_batch=True,        \n",
        "                    repeat=False)                  \n",
        "#best model from hyperparameter testing\n",
        "model = RNN(256,0.5)\n",
        "model.load_state_dict(torch.load('/content/backup/model_RNN_bs32_lr0.001_epochs20_hidden256_dropout0.5/best.pth'))\n",
        "model.to(device)\n",
        "false_positive_rate = 1 - get_accuracy(model, val_nospam)\n",
        "false_negative_rate = 1 - get_accuracy(model, val_spam)\n",
        "\n",
        "print(f\"False positive rate on val dataset is {false_positive_rate*100:.3f}%\")\n",
        "print(f\"False negative rate on val dataset is {false_negative_rate*100:.3f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1iRteb3a91O"
      },
      "source": [
        "### Part (e) [2 pt]\n",
        "\n",
        "The impact of a false positive vs a false negative can be drastically different.\n",
        "If our spam detection algorithm was deployed on your phone, what is the impact\n",
        "of a false positive on the phone's user? What is the impact of a false negative?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "False positive would mean the algorithm detects it as spam when it is not, leading to important messahes being missed. This is much worse than a false negative, where the alogrithm would flag a spam message as a normal message, resulting in some spam messages in inbox."
      ],
      "metadata": {
        "id": "IoovN-4OvGnf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gznefulsa91V"
      },
      "source": [
        "## Part 4. Evaluation [11 pt]\n",
        "\n",
        "### Part (a) [1 pt]\n",
        "\n",
        "Report the final test accuracy of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5L5D-A1a91W",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c1d04e-da39-462f-cd8c-91895cf1c749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is 97.846%\n"
          ]
        }
      ],
      "source": [
        "#since model is loaded...\n",
        "\n",
        "test = torchtext.legacy.data.BucketIterator(\n",
        "                    test_data,\n",
        "                    batch_size=1,\n",
        "                    sort_key=lambda x: len(x.sms), \n",
        "                    sort_within_batch=True,        \n",
        "                    repeat=False)\n",
        "\n",
        "test_accuracy = get_accuracy(model, test)\n",
        "print(f\"Test accuracy is {test_accuracy*100:.3f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hjmd8rca91Y"
      },
      "source": [
        "### Part (b) [3 pt]\n",
        "\n",
        "Report the false positive rate and false negative rate of your model across the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFiAKztJa91Z",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40db03b9-8321-4506-cdf6-51a19877ee58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False positive rate on test dataset is 1.320%\n",
            "False negative rate on test dataset is 8.527%\n"
          ]
        }
      ],
      "source": [
        "#since model is loaded...\n",
        "\n",
        "test_spam = torchtext.legacy.data.Dataset(\n",
        "    [e for e in test_data.examples if e.label == 1],\n",
        "    test_data.fields)\n",
        "test_nospam = torchtext.legacy.data.Dataset(\n",
        "    [e for e in test_data.examples if e.label == 0],\n",
        "    test_data.fields)\n",
        "\n",
        "test_spam_loader= torchtext.legacy.data.BucketIterator(\n",
        "                    test_spam,\n",
        "                    batch_size=1,\n",
        "                    sort_key=lambda x: len(x.sms), # to minimize padding\n",
        "                    sort_within_batch=True,        # sort within each batch\n",
        "                    repeat=False)                  # repeat the iterator for many epochs\n",
        "\n",
        "test_nospam_loader = torchtext.legacy.data.BucketIterator(\n",
        "                    test_nospam,\n",
        "                    batch_size=1,\n",
        "                    sort_key=lambda x: len(x.sms), # to minimize padding\n",
        "                    sort_within_batch=True,        # sort within each batch\n",
        "                    repeat=False)                  # repeat the iterator for many epochs\n",
        "\n",
        "test_false_positive_rate = 1 - get_accuracy(model, test_nospam_loader)\n",
        "test_false_negative_rate = 1 - get_accuracy(model, test_spam_loader)\n",
        "print(f\"False positive rate on test dataset is {test_false_positive_rate*100:.3f}%\")\n",
        "print(f\"False negative rate on test dataset is {test_false_negative_rate*100:.3f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jGHtQFpa91b"
      },
      "source": [
        "### Part (c) [3 pt]\n",
        "\n",
        "What is your model's prediction of the **probability** that\n",
        "the SMS message \"machine learning is sooo cool!\" is spam?\n",
        "\n",
        "Hint: To begin, use `text_field.vocab.stoi` to look up the index\n",
        "of each character in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_2nSJq8a91b",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d791061-a669-4dae-8d19-24bfcadda113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-5.1835, device='cuda:0', grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#since model is loaded...\n",
        "\n",
        "msg = \"machine learning is sooo cool!\"\n",
        "message = torch.tensor([[text_field.vocab.stoi[letter] for letter in msg]])\n",
        "pred = model(message)\n",
        "print(pred)\n",
        "\n",
        "#prob_of_spam = np.exp(float(pred[0][1])) / (np.exp(float(pred[0][0])) + np.exp(float(pred[0][1])))\n",
        "#print(\"probability of the message to be spam is:\", prob_of_spam)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the prediction is negative, we know that the model predicts this as not a spam message."
      ],
      "metadata": {
        "id": "jVRGRa366xeQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD1zgYJpa91f"
      },
      "source": [
        "### Part (d) [4 pt]\n",
        "\n",
        "Do you think detecting spam is an easy or difficult task?\n",
        "\n",
        "Since machine learning models are expensive to train and deploy, it is very\n",
        "important to compare our models against baseline models: a simple\n",
        "model that is easy to build and inexpensive to run that we can compare our\n",
        "recurrent neural network model against.\n",
        "\n",
        "Explain how you might build a simple baseline model. This baseline model\n",
        "can be a simple neural network (with very few weights), a hand-written algorithm,\n",
        "or any other strategy that is easy to build and test.\n",
        "\n",
        "**Do not actually build a baseline model. Instead, provide instructions on\n",
        "how to build it.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detecting spam is a difficult task. Increasingly, spam messages try to emulate real-life messages, making it ever harder for the people to distinguish between the two. \n",
        "To create a baseline model, a simple classifier which uses the different words used in ham and spam sets, and creating a probability of each word being in a spam versus a non-spam message. After this, we can mulitply the probabilities of the words to obtain a final probability which determine if they are spam or not."
      ],
      "metadata": {
        "id": "QX5iTqVW2II9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lab_5_Spam_Detection_Winter_2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}